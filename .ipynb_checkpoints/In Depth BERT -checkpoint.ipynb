{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Dhaka\n",
    "\n",
    "#### Department of Computer Science and Engineering\n",
    "\n",
    "**Natural Language Processing**\n",
    "\n",
    "4th Year 2nd Semester\n",
    "\n",
    "Session : 2019 -20\n",
    "\n",
    "**Topic**: ***BERT***\n",
    "\n",
    "**Submitted by:**\n",
    "\n",
    "Md Fahim (FH-26)\n",
    "\n",
    "Md Sakib Khan (AE-45)\n",
    "\n",
    "*Submission Date: 09-02-2021*\n",
    "\n",
    "**Submitted to:**\n",
    "\n",
    "*Dr. Asif Hossain Khan*, \n",
    "\n",
    "Professor,\n",
    "\n",
    "Department of Computer Science and Engineering,\n",
    "\n",
    "University of Dhaka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Table of Contents\n",
    "\n",
    "  *i. BERT, What is BERT !*\n",
    "\n",
    "\n",
    "  *ii. High Level Overview of BERT*\n",
    "\n",
    "\n",
    "  *iii. How BERT comes and why it becomes so popular ?*\n",
    "\n",
    "   - Limitation of Transformer\n",
    "   - Transfer Learning of NLP\n",
    "   - Fully Replacement of LSTM\n",
    "\n",
    "\n",
    "  *iv. What is inside the BERT ?*\n",
    "\n",
    "   - From Word to Vectors\n",
    "   - Encoder from the Transformer \n",
    "   - Maksed Language Modeling\n",
    "   - Next Sentence Prediction \n",
    "\n",
    "\n",
    "   *v. BERT as Transfer Learning in NLP*\n",
    "\n",
    "\n",
    "  *vi. BERT for Different NLP tasks*\n",
    "\n",
    "   - Sentence Pair Classification Task\n",
    "   - Single Sentence Classfication Task\n",
    "   - Question Answering Task\n",
    "   - Single Sentence Tagging Task\n",
    "\n",
    "\n",
    "  *vii. Applications* \n",
    "\n",
    "\n",
    " *viii. References* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to talk about bert.\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;$ \n",
    "\n",
    "<img src=\"Asol_Bert.jpg\" alt=\"Girl in a jacket\" width=100 height=100 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "No, no, we are going to talk about Google BERT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT, What is BERT !\n",
    "\n",
    "**BERT** (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers) is awesome research in Natural Language Processing (NLP) published by researchers at Google AI Language in 2018. It has caused a stir in the Machine Learning community by presenting state-of-the-art results in a wide variety of NLP tasks, including Neural Machine Translation, Question Answering (SQuAD v1.1), Sentence Pair Classification task (MNLI), Sentiment Analysis, Text Summarization and others.\n",
    "From the abbreviation of the BERT, we can figure out some kind of feature of it.\n",
    "\n",
    "   1. It is Bi-directional \n",
    "   2. It uses an Encoder Representation \n",
    "   3. It has a Transformer based architecture \n",
    "\n",
    "So basically, BERT’s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modeling. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training. The paper’s results show that a language model that is bi-directionally trained can have a deeper sense of language context and flow than single-direction language models. In the paper, the researchers detail a novel technique named Masked Language Modelling (MLM) which allows bidirectional training in models in which it was previously impossible. We will walk through all the details later in this material.\n",
    "\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ \n",
    "\n",
    "<img src = \"Bert_2.png\" width = 350 height = 450 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Level Overview of BERT\n",
    "\n",
    "Now, let's look at a *high-level* overview of the **BERT architecture**. It is a **Transformer** based model architecture, which opens a new era to work with NLP tasks. In the last session, we learned about the Transformer. A transformer is an Encoder-Decoder model architecture that also uses positional encoding, self-attention, multi-head attention, and also with Residual connection.\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ \n",
    "\n",
    "<img src = \"Transformer_2.png\" width = 400 height = 400 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ \n",
    "\n",
    "Now, BERT uses only the **Encoder** portion of the Transformer architecure canceling out the Decoder part.Like the Transformer BERT also uses postional encoding, self attention, multi head attention and Residual Connection. It uses exactly same architecture for encoders that is used in the Transformer. In short BERT is basically, **Stacking of Encoders** and the encoder is from the Transformer.  \n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ \n",
    "\n",
    "<img src = \"stack_of_encoders.png\" width = 700 height = 400 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ \n",
    "\n",
    "There are two types of BERT.\n",
    " 1. $BERT$ $base$\n",
    " 2. $BERT$ $large$\n",
    "\n",
    "\n",
    "In bert_base, it is used tweleve (12) encoder stacking. On the other hand, in bert_large twenty four (24) encoder stacking is used. The types are also different from feedforward-networks (no. of hidden neurons). For the bert_base , 768 neurons are used in the feedforward network where the bert_large uses 1024 hidden units.  \n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ \n",
    "\n",
    "<img src = \"type_bert.png\" width = 600 height = 350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How BERT comes and why it becomes so popular ?\n",
    "\n",
    "Nowadays, BERT has been the first choice to work on solving different NLP tasks. Why has it become so popular in the NLP community? It is not just for the being State Of The Art (SOTA) model but also several reasons behind it. The reasons can be differentiated into three main reasons.\n",
    "        \n",
    "   1. Limitations of Transformer \n",
    "   2. Transfer Learning in NLP\n",
    "   3. Fully replacement of LSTM\n",
    " \n",
    "\n",
    "### 1. Limitations of Transformer\n",
    "\n",
    "No doubt, the invention of the transformer in NLP is one of the most powerful and efficient research in the NLP community. It describes the NLP task in a different way for which the NLP model can be trained independently. As a result, **parallelization** is possible and so the training process becomes much faster than LSTM/GRU-based models. It has achieved LSTM/GRU models on different tasks such as machine translation started to make some in the field think of them as a replacement to LSTMs. This was compounded by the fact that Transformers deal with long-term dependencies better than LSTMs.\n",
    "\n",
    "\n",
    "Ok, it is well designed for the encoder-decoder architecture which is able to solve some NLP tasks like **Machine Translation**. But there are many more tasks which do not use an encoder-decoder architecture just like **Text Classification, Sentence Pair Task** and so on. On that task, how can we use transformers on those tasks? Here is the limitation of the transformer. For this limitation, we can't replace LSTM\n",
    "\n",
    "\n",
    "### 2.  Transfer Learning in NLP\n",
    "\n",
    "**Transfer learning is the reuse of a pre-trained model on a new problem**. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data. This is very useful in the data science field since most real-world problems typically do not have millions of labeled data points to train such complex models.\n",
    "\n",
    "\n",
    "Transfer learning is used not only for the faster training process but also for building an efficient model. In computer vision tasks, transfer learning is widely used. A robust model is trained on a huge amount of image data and finds the optimal weights. Then, the model can be used for downstream computer vision tasks as a pre-trained model. So, by adding some NN layers and fine-tuning the pre-trained model, it is easy to build an efficient model on that downstream task. Some pre-trained models in computer vision are VGG-16, ResNet, ImageNet, LeNet, Efficient net, and many more.\n",
    "\n",
    "\n",
    "For computer vision, we have a very good set of well-trained models on millions of data and they can be used easily to perform object recognition tasks. We can build *a robust and very accurate model with 20 lines of code*. Just  importing a pre-trained model and fine-tuning a few layers will give us the desired result.\n",
    "\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ \n",
    "\n",
    "<img src = \"transfer_learning.png\" width = 400 height = 300 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ \n",
    "\n",
    "Though transfer learning is much more popular and widly using model, but in NLP we've seen very less use of this.For NLP, the process is more complicated, because in NLP different text data contains different contexts. So, to use the transfer larning we need a good contextual language modeling.Contextual language modeling means the context of the sentence (for example, *abide by* and *abide in* have fully different meaning though both use the main word *abide*). More better contextual language modeling we can define, more better model we can build using transfer learning. After many research, some good contextual language model was built like **ELMo** , **ULMFiT** which were become popular. But both of **ELMo**( **E**mbeddings from **L**anguage **Mo**dels) and **ULMFiT**( **U**niversal **L**anguage **M**odel **Fi**ne-**T**uning) were LSTM based. In ELMo, 2 layers of bi-directional LSTM were used and in ULMFit 3 layers of LSTM was used.\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ \n",
    "\n",
    "<img src = \"ULMFit_2.png\" width = 500 height = 350 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ \n",
    "\n",
    "\n",
    "Here $E_1$, $E_2$, ... $E_N$ is the word embedding and $T_1$, $T_2$, ... $T_N$ is the output of the model\n",
    "\n",
    "$\\;\\;\\$ \n",
    "\n",
    "<img src = \"elmo.png\" width = 400 height = 300 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ \n",
    "\n",
    "But, what if we can bulid a Language Modeling with transformer in place of LSTM. As transformer is much more faster than LSTM , so we can train much more data to train by which we can define much more better language model. Bascially, a transformer based Language Modeling is needed which also can be used as pre-training model in transfer learning. \n",
    "\n",
    "Before BERT, OpenAI introduces their GPT-2 ( **G**enerative **P**re-**T**raining ) model which can solve the problem discussed in previous section. GPT-2 model is transformer based. If we look at the architecture of GPT-2 we will see, GPT-2 use 12 layer stacking of decoder ( unlike BERT, it uses only the decoder ). Encoder of the transformer is cancel out in the model. As, the model use only the decoders so it also cancel out the **Encoder - Decoder Attention** block from the decoder architecture. The rest are remain same. \n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ \n",
    "\n",
    "<img src = \"gpt-2.png\" width = 300 height = 100 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "We already know that decoder in the transformer uses **masked self attention** to predict the next word for learning. That's why decoder always gives shifted right word as prediciton. It works like the forward LSTM ( from left to right ). As GPT-2 is able to predict the next word for any sentece in the autoregressive way using transformer, so making a language modeling is possible with transformer. \n",
    "\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ \n",
    "\n",
    "\n",
    "Here $E_1$, $E_2$, ... $E_N$ is the word embedding, $T_1$, $T_2$, ... $T_N$ is the output of the model and **Trm** is Transformer Decoder\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ \n",
    "\n",
    "\n",
    "<img src = \"gpt2_arch.png\" width = 300 height = 200 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "GPT-2 is used as a pre-trained model in transfer learning using transformer. It also becomes able to use the transformer in different NLP tasks. Here is the big picture how to use GPT-2 ( Tansformer ) in different NLP task. For more about GPT-2, you can explore [their blogs. ](https://openai.com/blog/tags/gpt-2/) \n",
    "\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ \n",
    "\n",
    "\n",
    "<img src = \"GPT_nlp_task.jpg\" width = 550 height = 450 />\n",
    "\n",
    "\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Fully Replacement of LSTM\n",
    "\n",
    "In the previous section, we've seen that OpenAI GPT-2 becomes able to replace LSTM in many NLP tasks using transformer. Then, one important question arises that actually GPT-2 is able to replace LSTM totally ? The answer is **NO**. Because, while we using LSTM in our task, we can build more robust and contextual languge modeling using bi-directionality. **Bi-directional LSTM** understands better context in language modeling. As GPT-2 uses only the forward direction ( from left to right ), in some case **Bi-directional LSTM** works much more better by understanding better context of the language. So, we could not replace LSTM fully. Now, there araises one question is it possible to build a model adding **bi-directionality** using the **transformer ?** If it could be done, LSTM would fully replaced by the transformer.\n",
    "\n",
    "So, there required to build one model which \n",
    "\n",
    "   1. **Is capable to use tranformer in all kinds of NLP task ?**\n",
    "   2. **Can be used as a pre trained model for the downstream task while using transfer learning in NLP ?**\n",
    "   3. **Is bi-directional and understands the language form left to right and right to left ?**\n",
    "   4. **Understands deeper and better context of the language**\n",
    "     \n",
    "\n",
    "\n",
    "### Then **BERT** said, \n",
    "\n",
    "\n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    " \n",
    " <img src = \"kgf-2.png\" width = 250 height = 250 />\n",
    " \n",
    " $\\;\\;\\;\\;\\;\\;$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is inside the BERT ?\n",
    "\n",
    "We've already seen the basic overview of **BERT**. Basically , BERT is the stacking of **Encoders**. Now, it is high time to explain why they use only the encoder ? It is because to use BERT in all kinds of task. Another reason is that we've seen that OpenAI GPT-2 use stacking of decoders. As a result, they would build their model in auto-regressive model (next word prediction). So, they was able to define unidirection (left to right) transformer architecture. To build bi-directional transformer architecture BERT uses encoder from the transformer instead of the decoders. So, BERT build a model for not only left to right but also right to left. A simple overview of BERT is like :-\n",
    "\n",
    "\n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    " \n",
    " <img src = \"arch_bert.png\" width = 400 height = 400 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "\n",
    "Now, what is inside the BERT ? It is not just the stacking of encoders but also many more. BERT also introduces masked language modeling and next sentence prediciton. There are four major parts of BERT.\n",
    "\n",
    " \n",
    "          1. From Word to Vectors\n",
    "          2. The Encoders from the transformer\n",
    "          3. Masked Language Modeling (MLM)\n",
    "          4. Next Sentence Prediction (NSP)\n",
    "      \n",
    " \n",
    "It is high time to give a berife description of those parts. First of all, we need to know that BERT takes two sentences as input at a time. Let say those are sentence A and sentence B "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Word to Vectors\n",
    "\n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    " \n",
    " <img src = \"Num_To_Vec.png\" width = 900 height = 800 />\n",
    " <img src = \"num_to_vec_2.png\" width = 900 height = 800 />\n",
    " \n",
    "- **Tokenization** is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.\n",
    "\n",
    "$\\;\\;\\;$\n",
    "\n",
    "- **Using wordpieces** (e.g. playing -> play + ##ing) instead of words. This is effective in reducing the size of the vocabulary and increases the amount of data that is available for each word.\n",
    "\n",
    "$\\;\\;\\;$\n",
    "\n",
    "\n",
    "- **Numericalization** aims at mapping each token to a unique integer in the corpus’ vocabulary.\n",
    "\n",
    "$\\;\\;\\;$\n",
    "\n",
    "\n",
    "- **Token embedding** is the task of get the embedding (i.e. a vector of real numbers) for each word in the sequence. Each word of the sequence is mapped to a emb_dim dimensional vector that the model will learn during training. You can think about it as a vector look-up for each token. The elements of those vectors are treated as model parameters and are optimized with back-propagation just like any other weights.\n",
    "\n",
    "$\\;\\;\\;$\n",
    "\n",
    "- **Padding** was used to make the input sequences in a batch have the same length. That is, we increase the length of some of the sequences by adding ‘<pad>’ tokens.\n",
    "\n",
    "$\\;\\;\\;$\n",
    "\n",
    "\n",
    "- **Positional encoding** Recall that the positional encoding is designed to help the model learn some notion of sequences and relative positioning of tokens. This is crucial for language-based tasks especially here because we are not making use of any traditional recurrent units such as RNN, GRU or LSTM\n",
    "\n",
    " Intuitively, we aim to be able to modify the represented meaning of a specific word depending on its position. We don’t want to change the full representation of the word but we want to modify it a little to encode its position by adding numbers between [-1,1] using predetermined (non-learned) sinusoidal functions to the token embeddings. For the rest of the Encoder, the word will be represented slightly differently depending on the position the word is in (even if it is the same word).\n",
    "\n",
    " Encoder must be able to use the fact that some words are in a given position while, in the same sequence, other words are in other specific positions. That is, we want the network to able to understand relative positions and not only absolute ones.\n",
    " \n",
    " Now, the function use for the positional encoding is given below:\n",
    " \n",
    "  $\\;\\;\\;\\;\\;\\;$\n",
    " <img src = \"pos_enc.png\" width = 550 height = 550 />\n",
    "  $\\;\\;\\;\\;\\;\\;$\n",
    "  \n",
    "  Here, several questions arise like why the function use sinusoidal functions, why the function does not use single sin or cos function and finally why this encoding is added up with the encoding instead of concatination ? First of all, to encode anything binary encoding is the first choice. But all we know that all the weight, bias and the others of a NN model ranges from [-1, 1]. So, if we want to use binary encoding for this then we need float continous values to encode.  Obviously , binary encoding of the floating values is much more complex and very much waste of space. To solve thus condition, they use sinusoidal function.   \n",
    "  \n",
    "  Besides, the function use a combination of sin and cos, because it works equivalent to alternative bits when we use binary encoding. Single sin or single cos can not do this. For example, for **pos = 0** and for **index = 0,1** the fucntion gives us **0,1** respectively which just like the alternative bits in binary encoding. Finally, this posiitonal encoding is summed up with the embedding representation instead of concatination just because providing a good source of features and to store a smaller dim vector. ( if we concatenate, we need to store higher dimensionality vectors )  \n",
    "  \n",
    "   $\\;\\;\\;\\;\\;\\;$\n",
    "  \n",
    "- **Sentence embedding** techniques represent entire sentences and their semantic information as vectors. This helps the machine in understanding the context, intention, and other nuances in the entire text. It simply notices that which word belong to which sentences. A marker indicating Sentence A or Sentence B is added to each token. This allows the model to distinguish between sentences. \n",
    "  \n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    " \n",
    "### After the first step, we done our embedding and encoding step just like\n",
    "    \n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"embedding.png\" width = 550 height = 550 />\n",
    " \n",
    " $\\;\\;\\;\\;\\;\\;$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Encoders from the transformer\n",
    "\n",
    "The transformer architecture is based on encoder-decoder form. The encoder consists of  four parts ( self attention, multi-head attention, residual connections and normalization and feed forward network ). The encoder of the transformer architecture looks like\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"encoder.png\" width = 150 height = 150 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "In the encoder architecture we've seen that there are four measure part in the encoder part of the transformer. To understand the whole big picture of the encoder architecture of the transformer the following topics need to understand \n",
    "\n",
    "        i.   Attention Score Measurement\n",
    "        ii.  Self Attention and its intuition\n",
    "        iii. Multi Head Attention/ Self Attention\n",
    "        iv.  Add & Norm\n",
    "         v.  Feed Forward Network\n",
    "        \n",
    " Those topics are already covered. Let see a quick recap on those topics.\n",
    " \n",
    " - **i. Attention Score Measurement**\n",
    "  \n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "The attention mechanism is a part of a neural architecture that enables to dynamically highlight relevant features of the input data, which, in NLP, is typically a sequence of textual elements. It can be applied directly to the raw input or to its higher level representation. There are several functions used for measuring attention score ( e.g additive attention score, dot product attention score ). The most common is using dot product attention score by generating query, key , value vector. The function is used for measuring such attention score is given below\n",
    "\n",
    "\n",
    "<img src = \"attention.png\" width = 500 height = 600 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "- **ii.  Self Attention and its intuition**\n",
    "\n",
    "**Self-attention**, also known as **intra-attention**, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence.While doing self-attention in transformer, we follow few steps\n",
    "\n",
    "   - Create three vectors from each of the encoder’s input vectors\n",
    "   - For each word, we create a Query vector, a Key vector, and a Value vector\n",
    "   - These vectors are created by multiplying the embedding by three matrices that we trained during the training process.\n",
    "   \n",
    "   \n",
    "We need to generate three vectors ($Q$,$K$ and $V$) for each word embedding because we need to find the attention score for each word with all the words in the input sequence where the word belongs to.\n",
    "We will generate three vectors ($Q$,$K$ and $V$) for each word embedding by multiplying the word embedding with three weight metrics ($W_Q$,$W_K$,$W_V$). The metrics will be learned by model via backpropagation.  \n",
    " \n",
    "Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have a dimensionality of 512.\n",
    " \n",
    "**Why is dimensionality 64?**\n",
    "\n",
    "As we must have :\n",
    "\n",
    "   - Output’s dimension is [length of input sequences] x [dimension of embeddings — 512]\n",
    "\n",
    "   - We use 8 heads during the Multi-head Self-Attention process. The output size of a given self-attention vector is [length of input sequences] x [64]. So the concatenated vector resulting from all Multi-head Self-Attention process would be [length of input sequences] x ([64] x [8]) = [length of input sequences] x ([512])\n",
    "\n",
    "\n",
    "So, we will get a 64 dimension query, key, and value vector for each word.\n",
    "\n",
    "  \n",
    "  $\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"self_attention.png\" width = 1000 height = 1000 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "  \n",
    "  $\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"self_attention_2.png\" width = 600 height = 600 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "- **iii. Multi Head Attention**\n",
    "\n",
    "Multi-head attention means keeping attention to multiple words in a sequence for a single word. Multi-head attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel. Intuitively, multiple attention heads allows for attending to parts of the sequence differently ( e.g. longer term dependencies versus shorter-term dependencies ).\n",
    "\n",
    "**Why do we need multi head attention ?**\n",
    "\n",
    "While we are doing self attention, we will observe that a word by itself gets much more attention rather than the attention score with the other words. It may hamper the model to understand the context. So if we measure the self attention score multiple times we will reduce this problem a little bit. \n",
    "\n",
    "  \n",
    "  $\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"multi_head.png\" width = 200 height = 200 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "  \n",
    "  $\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"multi_head_2.png\" width = 900 height = 900 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Till now the whole picture is like that \n",
    "\n",
    "\n",
    "  $\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"whole_pic.png\" width = 900 height = 900 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "\n",
    "  $\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"whole_pic_2.png\" width = 900 height = 900 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "\n",
    "- **iv. Add and Norm** \n",
    " \n",
    " Here Add means the residual connection and norm mean the layer normalization. It also use dropout in this layer to reduce overfitting. \n",
    "\n",
    "$\\;\\;$\n",
    "\n",
    "$$\n",
    "\\text{ Layer Norm }(x+\\text{Dropout}(\\text{ Sublayer}(x)))\n",
    "$$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "To have a clear look let consider the following example\n",
    "\n",
    "\n",
    "  $\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"add_norm.png\" width = 500 height = 500 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "Here, Self attention is the sublayer and X = [x1, x2] , Z = [z1, z2]\n",
    "\n",
    "\n",
    "Now, Layernorm changes input to have mean 0 , variance 1 per layer and per training point (and adds two more parameters). The equation of the layer normalization\n",
    "\n",
    "$$\n",
    "\\mu^{l}=\\frac{1}{H} \\sum_{i=1}^{H} a_{i}^{l}\n",
    "$$\n",
    "\n",
    "$$\\quad h_{i}=f\\left(\\frac{g_{i}}{\\sigma_{i}}\\left(a_{i}-\\mu_{i}\\right)+b_{i}\\right)$$\n",
    "\n",
    "$$\\quad \\sigma^{l}=\\sqrt{ \\frac{1}{H} \\sum_{i=1}^{H}\\left(a_{i}^{l}-\\mu^{l}\\right)^{2} }$$\n",
    "\n",
    "\n",
    "\n",
    "Here $l$ is the layer, $\\mu$ is the mean, $\\sigma$ is the variance.\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "- **v. Feed Forward Network**\n",
    "\n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.This can be simplified by the equation\n",
    "\n",
    "$\\;\\;$\n",
    "\n",
    "$$\n",
    "\\operatorname{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2}\n",
    "$$\n",
    "\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Masked Language Modeling (MLM)\n",
    "\n",
    "One of the greatest feature of BERT is the Masked Language Modeling (MLM). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM objective allows the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer.\n",
    "\n",
    "\n",
    "Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. In technical terms, the prediction of the output words requires:\n",
    "\n",
    "     1. Adding a classification layer on top of the encoder output.\n",
    "     2. Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.\n",
    "     3. Calculating the probability of each word in the vocabulary with softmax.\n",
    "     \n",
    " \n",
    " \n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"mlm.png\" width = 600 height = 600 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "Here $w_1$,...$w_5$ are the word embedding and $O_1$,..$O_5$ are the outputs of the BERT. BERT uses GELU (Gaussian Error Linear Unit) activation.\n",
    "\n",
    "$$\n",
    "\\operatorname{GELU}(x):=x \\mathbb{P}(X \\leq x)=x \\Phi(x)=0.5 x\\left(1+\\operatorname{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right)\n",
    "$$\n",
    "\n",
    "The BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non-masked words. As a consequence, the model converges slower than directional models, a characteristic which is offset by its increased context awareness.\n",
    "\n",
    "Some important notes about are the following\n",
    "\n",
    "- **It is true bi-directional** \n",
    "  \n",
    "  Before BERT's MLM, bi-directional means just generating word representation going from left to right and right to left and then simply add or concate the two diretional's representation. For example, if we use bi-directional LSTM, one LSTM goes from left to right generating the representation of the words and other goes from right to left. After that it concatenates generated representation of the words. But it understands less context about the language. But in case of MLM , it masks a word within the text so no need to train the model from the both sides. Besides , the masked word is within the text, so BERT automatically follow those direction to the masked word from the other words \n",
    "  \n",
    "  $\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "\n",
    "<img src = \"mlm_2.jpg\" width = 500 height = 500 />\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "  \n",
    "  \n",
    "- **Semi Supervised Training**\n",
    "\n",
    "  As BERT randomly masks the word from the input text and learn from predicting the masked word, so there is no need to use supervised data with the label. So we can use both unsupervised and supervised data for training the BERT. As semi suprvised data can be used for training the BERT , so we get a huge amount of data for training. It will help us to build more robust and contexual model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Next Sentence Prediction (NSP)\n",
    "\n",
    "If we look back up at the input transformations the OpenAI transformer does to handle different tasks, you’ll notice that some tasks require the model to say something intelligent about two sentences (e.g. are they simply paraphrased versions of each other? Given a wikipedia entry as input, and a question regarding that entry as another input, can we answer that question?).\n",
    "\n",
    "To make BERT better at handling relationships between multiple sentences, the pre-training process includes an additional task: Given two sentences (A and B), is B likely to be the sentence that follows A, or not?\n",
    "\n",
    "In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.\n",
    "To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:\n",
    "- A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n",
    "- A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.\n",
    "- A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper.\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"embedding.png\" width = 550 height = 550 />\n",
    " \n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    " \n",
    " To predict if the second sentence is indeed connected to the first, the following steps are performed:\n",
    "- The entire input sequence goes through the Transformer model. ( input vector size is 768 for each word in BERT base) (**why 768?** the answer given below )\n",
    "- Each position outputs a vector of size hidden_size (768 in BERT Base). \n",
    "- The output of the [CLS] token is transformed into a 2×1 shaped vector, using a simple classification layer (learned matrices of weights and biases).\n",
    "- Calculating the probability of IsNextSequence (IsNext and NotNext labels) with softmax.\n",
    "\n",
    "\n",
    "For example,\n",
    "\n",
    "            Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]\n",
    "            Label = IsNext\n",
    "            Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]\n",
    "            Label = NotNext\n",
    "\n",
    "\n",
    "When training the BERT model, Masked LM and Next Sentence Prediction are trained together, with the goal of minimizing the combined loss function of the two strategies. Now, the whole BERT model in this looks like\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"nsp.png\" width = 750 height = 750 />\n",
    " \n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    " \n",
    " \n",
    " There is a small question **Why the input and the output of a single word holds a 764 dimesion vector in BERT_base model?**\n",
    " \n",
    " In BERT_base model, it uses 12 attention heads for multi-head attention. So, while we creating multi-head attention, we are using 12 heads for a single word. Each head contains 64 dimension key, query and value vector by which we get 64 dimesion vector with self attention score for a single word. \n",
    "         \n",
    "         So, the input and output vector of a single token/word will be = 12 X 64 = 768  dimesion embedding  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT as Transfer Learning in NLP\n",
    "\n",
    "As BERT takes unlabeled sentence pair and MLM understands the context of the language better so BERT can be trained on huge dataset which can be used as a pre-trained model in the downstrean tasks. The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus BERT uses the BooksCorpus (800M words) and English Wikipedia (2,500M words).For Wikipedia author extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences.\n",
    "\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"tl_bert.png\" width = 650 height = 650 />\n",
    " \n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "Overall pre-training and fine-tuning procedures for BERT given in the above picture. Apart from output layers, the same architectures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\n",
    "models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\n",
    "symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating questions/answers). Though BERT is trained on unsupervised data, BERT can be fine tuned on the supervised task.\n",
    "\n",
    "The whole picture if we use BERT as a pretrained model in a downstream supervised task ( let say question answering task ) will be look like:\n",
    "\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"tl_bert_2.png\" width = 750 height = 750 />\n",
    " \n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "If we want to use BERT for image classification the transfer learning process for BERT will be like \n",
    "\n",
    "\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"tl_bert_3.png\" width = 750 height = 750 />\n",
    " \n",
    " $\\;\\;\\;\\;\\;\\;$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT for Different NLP tasks\n",
    "\n",
    "We've already told that transformer directly can not use for different tasks but BERT can. How can we use BERT for different NLP tasks. Some of them described below\n",
    "\n",
    "\n",
    "- **a. Sentence Pair Classification task**\n",
    "\n",
    "    In sentence-pair classification, each example in a dataset has two sentences along with the appropriate target variable. E.g. Sentence similarity, entailment, etc. Sentence pairs are supported in all classification subtasks. There are many dataset like MNLI, QQP, SWAG etc on the sentence pair classification task. We can fine tune BERT in this task \n",
    "    \n",
    "    To solve this task, BERT takes input two sentences ( Sentence 1 and Sentence 2). Both sentence are separated by a [SEP] token and before those sentences a [CLS] token is inserted. Now, if we pass those sentences to the pre-trained BERT, the [CLS] token gives a probability by classifying those sentences. As, we are given the label output for the input, determining the loss by the label output we can train our sentence pair classification model using BERT.\n",
    "    \n",
    "    \n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"sen_pair.png\" width = 400 height = 400 />\n",
    " \n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    " \n",
    " \n",
    " - **b. Single Sentence Classification task**\n",
    " \n",
    "     In the single sentence classification task, we are given a sentence and asked to classify the sentence. For example, if we are given the body text of an E-Mail, then we need to predict wheather the E-Mail is spam or not. SST-2, CoLA are some dataset for this type of task.\n",
    "     \n",
    "     Now, to use BERT in this type of task, we pass a single sentence to the BERT input. In the begining of input (before the the sentence a [CLS] is insserted.Now, if we pass the sentence to the pre-trained BERT, the [CLS] token gives a probability by classifying the sentences. As, we are given the label output for the input, determining the loss by the label output we can train our single sentence classification model using BERT.\n",
    "     \n",
    "     \n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"ssct.png\" width = 400 height = 400 />\n",
    " \n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    " \n",
    " \n",
    " - **c.  Question Answering Task**\n",
    " \n",
    "     For the Question Answering task, BERT takes the input question and passage as a single packed sequence. The input embeddings are the sum of the token embeddings and the segment embeddings.\n",
    " \n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"Ques_ans.png\" width = 400 height = 400 />\n",
    " \n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    " \n",
    " \n",
    "   To fine-tune BERT for a Question-Answering system, it introduces a start vector and an end vector. The probability of each word being the start-word is calculated by taking a dot product between the final embedding of the word and the start vector, followed by a softmax over all the words. The word with the highest probability value is considered. A similar process is followed to find the end-word.\n",
    "    \n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"qa.png\" width = 400 height = 400 />\n",
    " \n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    " \n",
    " \n",
    "  $\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"start_tok.png\" width = 500 height = 500 />\n",
    " \n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    "    \n",
    "    \n",
    " - **d. Single Sentence Tagging Task**\n",
    " \n",
    "     In single sentence tagging tasks such as named entity recognition (where we are given a sentencr and we want fo find the name of anyperson/anything from the sentence)  , a tag must be predicted for every word in the input. The final hidden states (the transformer output) of every input token is fed to the classification layer to get a prediction for every token. Since WordPiece tokenizer breaks some words into sub-words, the prediction of only the first token of a word is considered.\n",
    " \n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"bert_sen_tag.PNG\" width = 400 height = 400 />\n",
    " \n",
    " $\\;\\;\\;\\;\\;\\;$\n",
    " \n",
    "  $\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "<img src = \"sstag.png\" width = 400 height = 400 />\n",
    " \n",
    " $\\;\\;\\;\\;\\;\\;$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications \n",
    "\n",
    "We have seen the magic of BERT. This is so robust a model that till now it is the most favorite model and also first choice model to solve any NLP task.BERT is undoubtedly a breakthrough in the use of Machine Learning for Natural Language Processing. The fact that it’s approachable and allows fast fine-tuning will likely allow a wide range of practical applications in the future. In this summary, we attempted to describe the main ideas of the paper while not drowning in excessive technical details.\n",
    "\n",
    "A different variation of BERT is now using in different real-life projects. Models trained on domain/application-specific corpus are Pre-trained models. Training on domain-specific corpus has shown to yield better performance when fine-tuning them on downstream NLP tasks like NER etc. for those domains, in comparison to fine tuning BERT. Some of the variations are listed below that are using different real-world NLP problem\n",
    "\n",
    "   - RoBERta (robustly optimized BERT for solving different tasks)\n",
    "   - BioBERT (use for biomedical text)\n",
    "   - SciBERT (use for scientific publications)\n",
    "   - ClinicalBERT (use for clinical notes)\n",
    "   - G-BERT (use for medical/diagnostic code representation and recommendation)\n",
    "   - M-BERT from 104 languages for zero-shot cross-lingual model transfer (task-specific annotations in one language is used to fine-tune a model for evaluation in another language)\n",
    "   - ERNIE (knowledge graph) + ERNIE (2) incorporates knowledge into pre-training but by masking entities and phrases using KG.\n",
    "   - TransBERT — unsupervised, followed by two supervised steps, for a story ending prediction task\n",
    "   - videoBERT (a model that jointly learns video and language representation learning) by representing video frames as special descriptor tokens along with text for pretraining. This is used for video captioning.\n",
    "   - DocBERT (use for Document classification)\n",
    "   - PatentBERT (Patent classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# References \n",
    "\n",
    "- BERT Paper - https://arxiv.org/pdf/1810.04805.pdf\n",
    "- OpenAI GPT2 - https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "- ULMFit paper - https://arxiv.org/abs/1801.06146\n",
    "- ELMo    - https://arxiv.org/abs/1802.05365\n",
    "- Transformer - https://arxiv.org/abs/1706.03762\n",
    "- Bidreactional RNN/LSTM - https://ieeexplore.ieee.org/document/650093\n",
    "- WordPieces Embedding - https://arxiv.org/abs/1609.08144\n",
    "- More About Transfromer - https://jalammar.github.io/illustrated-transformer/\n",
    "- Bert Explanined - https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "- Bert with hugging face - https://towardsdatascience.com/fine-tuning-a-bert-model-with-transformers-c8e49c4e008b\n",
    "- Understanding Word Embedding - https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598\n",
    "- Illustred bert - http://jalammar.github.io/illustrated-bert/\n",
    "- Inside Bert - https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1\n",
    "- Deconstructing Bert  - https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77\n",
    "- BERT in question answering - https://medium.com/saarthi-ai/build-a-smart-question-answering-system-with-fine-tuned-bert-b586e4cfa5f5\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
